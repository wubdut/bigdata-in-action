## 项目实践11：使用Spark处理实践数据

### step01 理解Spark Partition

datafile目录中有经过map reduce生成的特征数据集文件，将数据集分为训练集和测试集。步骤：
1）	将特征数据集拆分为多个数据块，限定每个数据块（partition）大小不超过5M；
2）	过滤每个数据块（partition）中的样本成为训练集和测试集，并写入到磁盘中；
3）	输出不同样本的结果到文件中。

### step02 理解Spark lazy

datafile目录中有经过map reduce生成的特征数据集文件，利用lazy统计特征数据集中负样本的数量（垃圾广告的数量）。步骤：
1)	将特征数据集拆分为多个数据块，限定每个数据块（partition）大小不超过5M；
2)	通过lazy操作省去中间文件写入磁盘的过程，直接对每个特征块的负样本进行进行统计；
3)	合并每个特征块中负样本的统计结果进行输出。

### step03 理解Spark shuffle的普通运行机制

datafile目录中有经过map reduce生成的特征数据集文件，统计不同设备对于广告的点击率（有些人就是爱看广告）。步骤：
1)	将特征数据集拆分为多个数据块，限定每个数据块（partition）大小不超过5M；
2)	将每个数据块（partition）内部进行排序和聚类；
3)	利用map数据结构开始对多个数据块进行开始聚类，当内存溢出是将文件写入磁盘中；
4)	对磁盘的文件进行合并，输出统计结果。

### step04 理解Spark shuffle的bypass运行机制

datafile目录中有经过map reduce生成的特征数据集文件，统计不同app类型对于广告的点击率（有些类型应用就是能推送客户喜欢的广告）。步骤：
1)	将特征数据集拆分为多个数据块，限定每个数据块（partition）大小不超过5M；
2)	将每个数据块（partition）内的数据根据key写入到磁盘中；
3)	对磁盘的文件进行合并，输出统计结果。
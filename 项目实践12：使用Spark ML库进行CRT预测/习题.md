#### 1. mllib支持的算法 
大体分为四大类，分类、聚类、回归、协同过滤。

#### 2. 描述kmeans算法原理

随机初始化中心点范围，计算各个类别的平均值得到新的中心点。 
重新计算各个点到中心值的距离划分，再次计算平均值得到新的中心点，直至各个类别数据平均值无变化。

#### 3. 请简要介绍下SVM

SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。

#### 4. 在k-means或kNN，一般用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？

曼哈顿距离只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。

#### 5. LR和SVM的联系与区别

##### 联系： 
1） LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2） 两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。 

##### 区别： 
1、LR是参数模型，SVM是非参数模型。 
2、从目标函数来看，区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 
3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 
5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。

#### 6. L1范数和L2范数的区别
L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 
比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|. 
简单总结一下就是： 
L1范数: 为x向量各个元素绝对值之和。 
L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数
Lp范数: 为x向量各个元素绝对值p次方和的1/p次方. 
在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 
L1范数可以使权值稀疏，方便特征提取。 
L2范数可以防止过拟合，提升模型的泛化能力。
